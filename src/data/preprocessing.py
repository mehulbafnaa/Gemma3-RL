"""
Preprocessing utilities for MathVista dataset with Gemma 3.
Focused on Gemma 3 multimodal format, answer/reasoning extraction,
normalization, correctness checking, and image preparation.
"""

import re
import logging
import sys
import numpy as np
from typing import Dict, List, Optional, Any
from PIL import Image, UnidentifiedImageError
from .prompt_template import create_mathvista_prompt, GemmaTokens
logger = logging.getLogger(__name__)

# Gemma 3 Special Tokens (adjust if necessary based on final model details)
BOS = "<bos>"
EOS = "<eos>"
START_OF_TURN = "<start_of_turn>"
END_OF_TURN = "<end_of_turn>"
USER = "user"
MODEL = "model"
IMAGE_TOKEN = "<img>" # Placeholder for where image context might be indicated


def format_gemma_prompt(
    question: str,
    image_provided: bool = False, # Indicate if an image is part of the input
    instruction: str = "Solve this problem step by step and give me all the reasoning."
) -> str:
    """
    Format a prompt for Gemma 3 multimodal input, following typical dialog structure.

    Args:
        question: Math question text.
        image_provided: Flag indicating if an image accompanies the question.
        instruction: Instruction to append to the question.

    Returns:
        Formatted prompt string for Gemma 3.
    """


    return create_mathvista_prompt(
        question=question,
        instruction=instruction,
        include_image=image_provided
    )


def format_gemma_response(
    content: str,
    include_tags: bool = True
) -> str:
    """
    Format a response string for Gemma 3, typically wrapping model output.

    Args:
        content: Response content generated by the model.
        include_tags: Whether to include the standard Gemma 3 model turn tags.

    Returns:
        Formatted response string.
    """
    content = content.strip()
    if include_tags:
        # Append EOS if it's meant to signify the end of the model's complete generation
        return f"{START_OF_TURN}{MODEL}\n{content}{END_OF_TURN}"
        # Consider adding EOS automatically: f"{START_OF_TURN}{MODEL}\n{content}{END_OF_TURN}{EOS}"
        # However, EOS handling is often managed by the generation pipeline itself.
    else:
        return content


def extract_answer(
    generation: str,
    answer_tag: str = "answer", # e.g., <answer> or [ANSWER]
    fallback_pattern: Optional[str] = r"final answer is[:\s]*(.*)",
) -> Optional[str]:
    """
    Extract the final answer from generated text, prioritizing specific tags.

    Args:
        generation: The full text generated by the model.
        answer_tag: The tag used to delimit the answer (e.g., 'answer' for <answer>...</answer>).
        fallback_pattern: A regex pattern to try if the primary tag is not found.

    Returns:
        The extracted answer string or None if no answer is found.
    """
    # Define regex pattern for the primary tag
    format_pattern = rf'<{answer_tag}>(.*?)</{answer_tag}>'
    match = re.search(format_pattern, generation, re.IGNORECASE | re.DOTALL)

    if match:
        return match.group(1).strip()

    # If primary tag fails, try extracting from within Gemma model tags
    model_turn_pattern = rf'{START_OF_TURN}{MODEL}(.*?){END_OF_TURN}'
    model_match = re.search(model_turn_pattern, generation, re.IGNORECASE | re.DOTALL)
    if model_match:
        content = model_match.group(1).strip()
        match = re.search(format_pattern, content, re.IGNORECASE | re.DOTALL)
        if match:
            return match.group(1).strip()
        # Try fallback within model content
        if fallback_pattern:
             fallback_match = re.search(fallback_pattern, content, re.IGNORECASE | re.DOTALL)
             if fallback_match:
                 return fallback_match.group(1).strip()

    # Try fallback on the entire generation string as a last resort
    if fallback_pattern:
        fallback_match = re.search(fallback_pattern, generation, re.IGNORECASE | re.DOTALL)
        if fallback_match:
            return fallback_match.group(1).strip()

    # Maybe look for the last line if no tags/patterns match? (Use with caution)
    # lines = generation.strip().split('\n')
    # if lines: return lines[-1].strip()

    logger.debug(f"Could not extract answer using tag '{answer_tag}' or fallback pattern from generation.")
    return None


def extract_reasoning(
    generation: str,
    reasoning_tag: str = "think", # e.g., <think> or [REASONING]
    answer_tag: str = "answer"
) -> Optional[str]:
    """
    Extract reasoning steps, often preceding the final answer tag.

    Args:
        generation: The full text generated by the model.
        reasoning_tag: The tag used to delimit reasoning steps.
        answer_tag: The tag used for the answer (to help isolate reasoning before it).

    Returns:
        Extracted reasoning string or None.
    """
    # Define regex patterns
    format_pattern = rf'<{reasoning_tag}>(.*?)</{reasoning_tag}>'
    model_turn_pattern = rf'{START_OF_TURN}{MODEL}(.*?){END_OF_TURN}'
    answer_pattern_str = rf'<{answer_tag}>.*?</{answer_tag}>'

    # 1. Try to find explicit reasoning tags within the model turn
    model_match = re.search(model_turn_pattern, generation, re.IGNORECASE | re.DOTALL)
    content_to_search = generation # Default to searching the whole generation
    if model_match:
        content_to_search = model_match.group(1).strip() # Focus within model tags

    reasoning_match = re.search(format_pattern, content_to_search, re.IGNORECASE | re.DOTALL)
    if reasoning_match:
        return reasoning_match.group(1).strip()

    # 2. If no explicit tag, try to extract content *before* the answer tag within the model turn
    answer_match_in_content = re.search(answer_pattern_str, content_to_search, re.IGNORECASE | re.DOTALL)
    if answer_match_in_content:
        # Get everything before the start of the answer match
        reasoning_part = content_to_search[:answer_match_in_content.start()].strip()
        if reasoning_part: # Check if there's actually content before the answer
             # Optional: Remove model start tag if present at the beginning
             reasoning_part = re.sub(rf'^{START_OF_TURN}{MODEL}\s*', '', reasoning_part).strip()
             return reasoning_part

    # 3. If still no reasoning, and we searched within model tags, maybe return the whole model content (minus answer)
    # This assumes the model output *is* the reasoning if no tags are used.
    if model_match and not answer_match_in_content: # Model content exists, but no answer found within it
         return content_to_search # Return the whole model content as reasoning

    logger.debug(f"Could not extract reasoning using tag '{reasoning_tag}' or implicitly before answer tag '{answer_tag}'.")
    return None # No reasoning found



def extract_answer_from_response(response: str, answer_tag: str = "answer") -> Optional[str]:
    """
    Extract the answer from a model's response using the specified tag.
    
    Args:
        response: The model's response text
        answer_tag: The tag used to mark the answer
        
    Returns:
        The extracted answer or None if not found
    """
    import re
    
    # Pattern to find content between tags
    pattern = rf'<{answer_tag}>(.*?)</{answer_tag}>'
    match = re.search(pattern, response, re.DOTALL | re.IGNORECASE)
    
    if match:
        return match.group(1).strip()
    
    # Fallback pattern for final answer without tags
    fallback_patterns = [
        r'final answer:?\s*(.*?)(?:$|\n)',
        r'answer:?\s*(.*?)(?:$|\n)',
        r'therefore,?\s*(.*?)(?:$|\n)'
    ]
    
    for pattern in fallback_patterns:
        match = re.search(pattern, response, re.DOTALL | re.IGNORECASE)
        if match:
            return match.group(1).strip()
    
    return None


# Update to the normalize_answer function in preprocessing.py

def normalize_answer(text: Optional[str]) -> str:
    """
    Normalize answer string for robust comparison.
    Handles lowercase, punctuation, whitespace, and basic number formatting.

    Args:
        text: Input answer string.

    Returns:
        Normalized string.
    """
    if text is None:
        return ""

    # Convert to lowercase
    text = str(text).lower()

    # Replace '^' with space before removing other punctuation
    text = text.replace('^', ' ')

    # Remove common units or symbols that might interfere
    text = re.sub(r"[\$%€£¥]", "", text)

    # Remove punctuation except decimal points and commas within numbers
    text = re.sub(r"[^a-z0-9\s.,-]", "", text)

    # Handle commas in numbers (remove them)
    text = re.sub(r"(\d),(\d)", r"\1\2", text)

    # Standardize whitespace - KEEP THIS LINE
    text = re.sub(r'\s+', ' ', text).strip()
    
    # Don't remove spaces completely - comment out or remove this line:
    # text = re.sub(r"\s+", "", text)

    return text

def check_answer_correctness(
    predicted: Optional[str],
    ground_truth: str,
    answer_type: str = "auto" # auto, text, number
) -> float:
    """
    Check if the predicted answer matches the ground truth, providing a score [0, 1].
    Includes exact match, numerical tolerance, and partial credit logic.

    Args:
        predicted: The predicted answer string from the model.
        ground_truth: The correct answer string.
        answer_type: Type hint for comparison ('auto', 'text', 'number').

    Returns:
        Score between 0.0 (incorrect) and 1.0 (correct).
    """
    if predicted is None:
        return 0.0

    # Normalize both predicted and ground truth answers
    norm_pred = normalize_answer(predicted)
    norm_gt = normalize_answer(ground_truth)

    # Additional space-insensitive normalization for math expressions
    # This handles cases like "25 pi" vs "25pi"
    math_norm_pred = norm_pred.replace(" ", "")
    math_norm_gt = norm_gt.replace(" ", "")


    if not norm_gt: # Cannot score if ground truth is empty after normalization
        logger.warning("Ground truth answer is empty after normalization. Cannot score.")
        return 0.0
    if not norm_pred: # Prediction is empty after normalization
        return 0.0

    # 1. Exact Match Check (most reliable)
    if norm_pred == norm_gt or math_norm_pred == math_norm_gt:
        return 1.0   

    # 1. Exact Match Check (most reliable)
    if norm_pred == norm_gt:
        return 1.0

    # Determine comparison type
    is_numeric_gt = False
    is_numeric_pred = False
    gt_num, pred_num = None, None

    try:
        gt_num = float(norm_gt)
        is_numeric_gt = True
    except ValueError:
        pass

    try:
        pred_num = float(norm_pred)
        is_numeric_pred = True
    except ValueError:
        pass

    use_numeric_comparison = (answer_type == "number") or \
                             (answer_type == "auto" and is_numeric_gt and is_numeric_pred)

    # 2. Numeric Comparison (if applicable)
    if use_numeric_comparison:
        # Check for near-equality for floats
        if abs(pred_num - gt_num) < 1e-5: # Tolerance for floating point
            return 1.0
        # Partial credit based on relative error (avoid division by zero)
        denominator = abs(gt_num) if abs(gt_num) > 1e-8 else 1.0
        rel_error = abs(pred_num - gt_num) / denominator
        if rel_error < 0.01: # Within 1%
             return 0.9
        elif rel_error < 0.05: # Within 5%
             return 0.7
        elif rel_error < 0.1:  # Within 10%
             return 0.5
        # No points if error is too large

    # 3. Text-based Partial Matching (if not an exact match or close number)
    # Consider substring check (gives high score, use carefully)
    # if norm_gt in norm_pred or norm_pred in norm_gt:
    #     return 0.7 # High partial credit for substring inclusion

    # Token Overlap (Jaccard Index or simple overlap)
    pred_tokens = set(norm_pred.split())
    gt_tokens = set(norm_gt.split())

    if not gt_tokens: return 0.0 # Avoid division by zero

    intersection = len(pred_tokens.intersection(gt_tokens))
    union = len(pred_tokens.union(gt_tokens))
    jaccard = intersection / union if union > 0 else 0.0

    # Simple overlap ratio based on ground truth length
    overlap_ratio = intersection / len(gt_tokens)

    # Assign score based on overlap, capped to avoid giving too much credit
    # Adjust thresholds and scores based on requirements
    if overlap_ratio > 0.8:
         score = 0.6
    elif overlap_ratio > 0.5:
         score = 0.4
    elif jaccard > 0.3: # Use Jaccard for less direct overlap
         score = 0.2
    else:
         score = 0.0

    # Ensure numeric comparison results aren't overwritten by lower text scores
    if use_numeric_comparison and score < 0.5: # If numeric failed significantly
        return 0.0 # Return 0 if numeric comparison was intended but failed

    return score


def resize_image_for_gemma(
    image_path: str,
    target_size: int = 896
) -> Optional[np.ndarray]:
    """
    Opens and resizes an image for Gemma 3 input, returning uint8 format.
    """
    try:
        img = Image.open(image_path).convert("RGB")
        img_resized = img.resize((target_size, target_size), resample=Image.Resampling.LANCZOS)
        
        # Return as uint8 (no normalization to [0,1])
        img_array = np.array(img_resized, dtype=np.uint8)
        
        return img_array
    except Exception as e:
        logger.error(f"Failed to resize image {image_path}: {e}", exc_info=True)
        return None